{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a ShoeBot Sales Agent using LangGraph and Graphiti\n",
    "\n",
    "The following example demonstrates building an agent using LangGraph. Graphiti is used to personalize agent responses based on information learned from prior conversations. Additionally, a database of products is loaded into the Graphiti graph, enabling the agent to speak to these products.\n",
    "\n",
    "The agent implements:\n",
    "- persistence of new chat turns to Graphiti and recall of relevant Facts using the most recent message.\n",
    "- a tool for querying Graphiti for shoe information\n",
    "- an in-memory MemorySaver to maintain agent state.\n",
    "\n",
    "## Install dependencies\n",
    "```shell\n",
    "pip install graphiti-core langchain-openai langgraph ipywidgets\n",
    "```\n",
    "\n",
    "Ensure that you've followed the Graphiti installation instructions. In particular, installation of `neo4j`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install graphiti-core langchain-openai langgraph ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from contextlib import suppress\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Annotated\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging():\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.ERROR)\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith integration (Optional)\n",
    "\n",
    "If you'd like to trace your agent using LangSmith, ensure that you have a `LANGSMITH_API_KEY` set in your environment.\n",
    "\n",
    "Then set `os.environ['LANGCHAIN_TRACING_V2'] = 'false'` to `true`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = 'false'\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'Graphiti LangGraph Tutorial'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Graphiti\n",
    "\n",
    "Ensure that you have `neo4j` running and a database created. Ensure that you've configured the following in your environment.\n",
    "\n",
    "```bash\n",
    "NEO4J_URI=\n",
    "NEO4J_USER=\n",
    "NEO4J_PASSWORD=\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Graphiti\n",
    "\n",
    "from graphiti_core import Graphiti\n",
    "from graphiti_core.edges import EntityEdge\n",
    "from graphiti_core.nodes import EpisodeType\n",
    "from graphiti_core.utils.maintenance.graph_data_operations import clear_data\n",
    "\n",
    "neo4j_uri = os.environ.get('NEO4J_URI', 'bolt://localhost:7687')\n",
    "neo4j_user = os.environ.get('NEO4J_USER', 'neo4j')\n",
    "neo4j_password = os.environ.get('NEO4J_PASSWORD', 'test1234')\n",
    "\n",
    "client = Graphiti(\n",
    "    neo4j_uri,\n",
    "    neo4j_user,\n",
    "    neo4j_password,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a database schema \n",
    "\n",
    "The following is only required for the first run of this notebook or when you'd like to start your database over.\n",
    "\n",
    "**IMPORTANT**: `clear_data` is destructive and will wipe your entire database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This will clear the database\n",
    "await clear_data(client.driver)\n",
    "await client.build_indices_and_constraints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Shoe Data into the Graph\n",
    "\n",
    "Load several shoe and related products into the Graphiti. This may take a while.\n",
    "\n",
    "\n",
    "**IMPORTANT**: This only needs to be done once. If you run `clear_data` you'll need to rerun this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphiti_core.llm_client.openai_base_client - ERROR - Error in generating LLM response: Error code: 404 - {'detail': 'Not Found'}\n",
      "graphiti_core.llm_client.openai_base_client - ERROR - Error in generating LLM response: Error code: 404 - {'detail': 'Not Found'}\n",
      "graphiti_core.llm_client.openai_base_client - ERROR - Error in generating LLM response: Error code: 404 - {'detail': 'Not Found'}\n",
      "graphiti_core.llm_client.openai_base_client - ERROR - Error in generating LLM response: Error code: 404 - {'detail': 'Not Found'}\n",
      "graphiti_core.llm_client.openai_base_client - ERROR - Error in generating LLM response: Error code: 404 - {'detail': 'Not Found'}\n",
      "graphiti_core.llm_client.openai_base_client - ERROR - Error in generating LLM response: Error code: 404 - {'detail': 'Not Found'}\n",
      "graphiti_core.llm_client.openai_base_client - ERROR - Max retries (2) exceeded. Last error: Error code: 404 - {'detail': 'Not Found'}\n",
      "graphiti_core.llm_client.openai_base_client - ERROR - Max retries (2) exceeded. Last error: Error code: 404 - {'detail': 'Not Found'}\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'detail': 'Not Found'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, product \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(products):\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39madd_episode(\n\u001b[1;32m     13\u001b[0m             name\u001b[38;5;241m=\u001b[39mproduct\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProduct \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     14\u001b[0m             episode_body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m({k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m product\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m}),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m             reference_time\u001b[38;5;241m=\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow(timezone\u001b[38;5;241m.\u001b[39mutc),\n\u001b[1;32m     18\u001b[0m         )\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m ingest_products_data(client)\n",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mingest_products_data\u001b[0;34m(client)\u001b[0m\n\u001b[1;32m      9\u001b[0m     products \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproducts\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, product \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(products):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39madd_episode(\n\u001b[1;32m     13\u001b[0m         name\u001b[38;5;241m=\u001b[39mproduct\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProduct \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     14\u001b[0m         episode_body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m({k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m product\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m}),\n\u001b[1;32m     15\u001b[0m         source_description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mManyBirds products\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m         source\u001b[38;5;241m=\u001b[39mEpisodeType\u001b[38;5;241m.\u001b[39mjson,\n\u001b[1;32m     17\u001b[0m         reference_time\u001b[38;5;241m=\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow(timezone\u001b[38;5;241m.\u001b[39mutc),\n\u001b[1;32m     18\u001b[0m     )\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/graphiti_core/graphiti.py:824\u001b[0m, in \u001b[0;36mGraphiti.add_episode\u001b[0;34m(self, name, episode_body, source_description, reference_time, source, group_id, uuid, update_communities, entity_types, excluded_entity_types, previous_episode_uuids, edge_types, edge_type_map)\u001b[0m\n\u001b[1;32m    822\u001b[0m span\u001b[38;5;241m.\u001b[39mset_status(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[1;32m    823\u001b[0m span\u001b[38;5;241m.\u001b[39mrecord_exception(e)\n\u001b[0;32m--> 824\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/graphiti_core/graphiti.py:741\u001b[0m, in \u001b[0;36mGraphiti.add_episode\u001b[0;34m(self, name, episode_body, source_description, reference_time, source, group_id, uuid, update_communities, entity_types, excluded_entity_types, previous_episode_uuids, edge_types, edge_type_map)\u001b[0m\n\u001b[1;32m    734\u001b[0m edge_type_map_default \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    735\u001b[0m     {(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntity\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[38;5;28mlist\u001b[39m(edge_types\u001b[38;5;241m.\u001b[39mkeys())}\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m edge_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m {(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntity\u001b[39m\u001b[38;5;124m'\u001b[39m): []}\n\u001b[1;32m    738\u001b[0m )\n\u001b[1;32m    740\u001b[0m \u001b[38;5;66;03m# Extract and resolve nodes\u001b[39;00m\n\u001b[0;32m--> 741\u001b[0m extracted_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m extract_nodes(\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclients, episode, previous_episodes, entity_types, excluded_entity_types\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    745\u001b[0m nodes, uuid_map, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m resolve_extracted_nodes(\n\u001b[1;32m    746\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclients,\n\u001b[1;32m    747\u001b[0m     extracted_nodes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    750\u001b[0m     entity_types,\n\u001b[1;32m    751\u001b[0m )\n\u001b[1;32m    753\u001b[0m \u001b[38;5;66;03m# Extract and resolve edges in parallel with attribute extraction\u001b[39;00m\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/graphiti_core/utils/maintenance/node_operations.py:148\u001b[0m, in \u001b[0;36mextract_nodes\u001b[0;34m(clients, episode, previous_episodes, entity_types, excluded_entity_types)\u001b[0m\n\u001b[1;32m    141\u001b[0m     llm_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m llm_client\u001b[38;5;241m.\u001b[39mgenerate_response(\n\u001b[1;32m    142\u001b[0m         prompt_library\u001b[38;5;241m.\u001b[39mextract_nodes\u001b[38;5;241m.\u001b[39mextract_text(context),\n\u001b[1;32m    143\u001b[0m         response_model\u001b[38;5;241m=\u001b[39mExtractedEntities,\n\u001b[1;32m    144\u001b[0m         group_id\u001b[38;5;241m=\u001b[39mepisode\u001b[38;5;241m.\u001b[39mgroup_id,\n\u001b[1;32m    145\u001b[0m         prompt_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextract_nodes.extract_text\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    146\u001b[0m     )\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m episode\u001b[38;5;241m.\u001b[39msource \u001b[38;5;241m==\u001b[39m EpisodeType\u001b[38;5;241m.\u001b[39mjson:\n\u001b[0;32m--> 148\u001b[0m     llm_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m llm_client\u001b[38;5;241m.\u001b[39mgenerate_response(\n\u001b[1;32m    149\u001b[0m         prompt_library\u001b[38;5;241m.\u001b[39mextract_nodes\u001b[38;5;241m.\u001b[39mextract_json(context),\n\u001b[1;32m    150\u001b[0m         response_model\u001b[38;5;241m=\u001b[39mExtractedEntities,\n\u001b[1;32m    151\u001b[0m         group_id\u001b[38;5;241m=\u001b[39mepisode\u001b[38;5;241m.\u001b[39mgroup_id,\n\u001b[1;32m    152\u001b[0m         prompt_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextract_nodes.extract_json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    153\u001b[0m     )\n\u001b[1;32m    155\u001b[0m response_object \u001b[38;5;241m=\u001b[39m ExtractedEntities(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mllm_response)\n\u001b[1;32m    157\u001b[0m extracted_entities: \u001b[38;5;28mlist\u001b[39m[ExtractedEntity] \u001b[38;5;241m=\u001b[39m response_object\u001b[38;5;241m.\u001b[39mextracted_entities\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/graphiti_core/llm_client/openai_base_client.py:216\u001b[0m, in \u001b[0;36mBaseOpenAIClient.generate_response\u001b[0;34m(self, messages, response_model, max_tokens, model_size, group_id, prompt_name)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m retry_count \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_RETRIES:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 216\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_response(\n\u001b[1;32m    217\u001b[0m             messages, response_model, max_tokens, model_size\n\u001b[1;32m    218\u001b[0m         )\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (RateLimitError, RefusalError):\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;66;03m# These errors should not trigger retries\u001b[39;00m\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/graphiti_core/llm_client/openai_base_client.py:145\u001b[0m, in \u001b[0;36mBaseOpenAIClient._generate_response\u001b[0;34m(self, messages, response_model, max_tokens, model_size)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response_model:\n\u001b[0;32m--> 145\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_structured_completion(\n\u001b[1;32m    146\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    147\u001b[0m             messages\u001b[38;5;241m=\u001b[39mopenai_messages,\n\u001b[1;32m    148\u001b[0m             temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemperature,\n\u001b[1;32m    149\u001b[0m             max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_tokens,\n\u001b[1;32m    150\u001b[0m             response_model\u001b[38;5;241m=\u001b[39mresponse_model,\n\u001b[1;32m    151\u001b[0m             reasoning\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreasoning,\n\u001b[1;32m    152\u001b[0m             verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbosity,\n\u001b[1;32m    153\u001b[0m         )\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_structured_response(response)\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/graphiti_core/llm_client/openai_client.py:81\u001b[0m, in \u001b[0;36mOpenAIClient._create_structured_completion\u001b[0;34m(self, model, messages, temperature, max_tokens, response_model, reasoning, verbosity)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Reasoning models (gpt-5 family) don't support temperature\u001b[39;00m\n\u001b[1;32m     77\u001b[0m is_reasoning_model \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     78\u001b[0m     model\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-5\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m model\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo1\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m model\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     79\u001b[0m )\n\u001b[0;32m---> 81\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mresponses\u001b[38;5;241m.\u001b[39mparse(\n\u001b[1;32m     82\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mmessages,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_reasoning_model \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     85\u001b[0m     max_output_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[1;32m     86\u001b[0m     text_format\u001b[38;5;241m=\u001b[39mresponse_model,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     reasoning\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meffort\u001b[39m\u001b[38;5;124m'\u001b[39m: reasoning} \u001b[38;5;28;01mif\u001b[39;00m reasoning \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     text\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m'\u001b[39m: verbosity} \u001b[38;5;28;01mif\u001b[39;00m verbosity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     89\u001b[0m )\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/openai/resources/responses/responses.py:2792\u001b[0m, in \u001b[0;36mAsyncResponses.parse\u001b[0;34m(self, text_format, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, prompt_cache_retention, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, verbosity, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   2785\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparser\u001b[39m(raw_response: Response) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ParsedResponse[TextFormatT]:\n\u001b[1;32m   2786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parse_response(\n\u001b[1;32m   2787\u001b[0m         input_tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m   2788\u001b[0m         text_format\u001b[38;5;241m=\u001b[39mtext_format,\n\u001b[1;32m   2789\u001b[0m         response\u001b[38;5;241m=\u001b[39mraw_response,\n\u001b[1;32m   2790\u001b[0m     )\n\u001b[0;32m-> 2792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   2793\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/responses\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2794\u001b[0m     body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m   2795\u001b[0m         {\n\u001b[1;32m   2796\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackground\u001b[39m\u001b[38;5;124m\"\u001b[39m: background,\n\u001b[1;32m   2797\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversation\u001b[39m\u001b[38;5;124m\"\u001b[39m: conversation,\n\u001b[1;32m   2798\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude\u001b[39m\u001b[38;5;124m\"\u001b[39m: include,\n\u001b[1;32m   2799\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2800\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m\"\u001b[39m: instructions,\n\u001b[1;32m   2801\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_output_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_output_tokens,\n\u001b[1;32m   2802\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tool_calls,\n\u001b[1;32m   2803\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   2804\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   2805\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   2806\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_response_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: previous_response_id,\n\u001b[1;32m   2807\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[1;32m   2808\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_cache_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_cache_key,\n\u001b[1;32m   2809\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_cache_retention\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_cache_retention,\n\u001b[1;32m   2810\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning,\n\u001b[1;32m   2811\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafety_identifier\u001b[39m\u001b[38;5;124m\"\u001b[39m: safety_identifier,\n\u001b[1;32m   2812\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   2813\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   2814\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   2815\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   2816\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   2817\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: text,\n\u001b[1;32m   2818\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   2819\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   2820\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   2821\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   2822\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncation\u001b[39m\u001b[38;5;124m\"\u001b[39m: truncation,\n\u001b[1;32m   2823\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   2824\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m\"\u001b[39m: verbosity,\n\u001b[1;32m   2825\u001b[0m         },\n\u001b[1;32m   2826\u001b[0m         response_create_params\u001b[38;5;241m.\u001b[39mResponseCreateParams,\n\u001b[1;32m   2827\u001b[0m     ),\n\u001b[1;32m   2828\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   2829\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers,\n\u001b[1;32m   2830\u001b[0m         extra_query\u001b[38;5;241m=\u001b[39mextra_query,\n\u001b[1;32m   2831\u001b[0m         extra_body\u001b[38;5;241m=\u001b[39mextra_body,\n\u001b[1;32m   2832\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   2833\u001b[0m         post_parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[1;32m   2834\u001b[0m     ),\n\u001b[1;32m   2835\u001b[0m     \u001b[38;5;66;03m# we turn the `Response` instance into a `ParsedResponse`\u001b[39;00m\n\u001b[1;32m   2836\u001b[0m     \u001b[38;5;66;03m# in the `parser` function above\u001b[39;00m\n\u001b[1;32m   2837\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast(Type[ParsedResponse[TextFormatT]], Response),\n\u001b[1;32m   2838\u001b[0m )\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/openai/_base_client.py:1794\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1782\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1789\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1790\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1791\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1792\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1793\u001b[0m     )\n\u001b[0;32m-> 1794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/openai/_base_client.py:1594\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1591\u001b[0m             \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1593\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1594\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'detail': 'Not Found'}"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "async def ingest_products_data(client: Graphiti):\n",
    "    # script_dir = Path.cwd().parent\n",
    "    # json_file_path = script_dir / 'data' / 'manybirds_products.json'\n",
    "    json_file_path = Path(\"..\") / \"data\" / \"manybirds_products.json\"\n",
    "\n",
    "    with open(json_file_path) as file:\n",
    "        products = json.load(file)['products']\n",
    "\n",
    "    for i, product in enumerate(products):\n",
    "        await client.add_episode(\n",
    "            name=product.get('title', f'Product {i}'),\n",
    "            episode_body=str({k: v for k, v in product.items() if k != 'images'}),\n",
    "            source_description='ManyBirds products',\n",
    "            source=EpisodeType.json,\n",
    "            reference_time=datetime.now(timezone.utc),\n",
    "        )\n",
    "\n",
    "\n",
    "await ingest_products_data(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a user node in the Graphiti graph\n",
    "\n",
    "In your own app, this step could be done later once the user has identified themselves and made their sales intent known. We do this here so we can configure the agent with the user's `node_uuid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphiti_core.llm_client.openai_base_client - ERROR - Error in generating LLM response: Error code: 404 - {'detail': 'Not Found'}\n",
      "graphiti_core.llm_client.openai_base_client - ERROR - Error in generating LLM response: Error code: 404 - {'detail': 'Not Found'}\n",
      "graphiti_core.llm_client.openai_base_client - ERROR - Error in generating LLM response: Error code: 404 - {'detail': 'Not Found'}\n",
      "graphiti_core.llm_client.openai_base_client - ERROR - Error in generating LLM response: Error code: 404 - {'detail': 'Not Found'}\n",
      "graphiti_core.llm_client.openai_base_client - ERROR - Error in generating LLM response: Error code: 404 - {'detail': 'Not Found'}\n",
      "graphiti_core.llm_client.openai_base_client - ERROR - Error in generating LLM response: Error code: 404 - {'detail': 'Not Found'}\n",
      "graphiti_core.llm_client.openai_base_client - ERROR - Max retries (2) exceeded. Last error: Error code: 404 - {'detail': 'Not Found'}\n",
      "graphiti_core.llm_client.openai_base_client - ERROR - Max retries (2) exceeded. Last error: Error code: 404 - {'detail': 'Not Found'}\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'detail': 'Not Found'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgraphiti_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msearch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msearch_config_recipes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NODE_HYBRID_SEARCH_EPISODE_MENTIONS\n\u001b[1;32m      3\u001b[0m user_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjess\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39madd_episode(\n\u001b[1;32m      6\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser Creation\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     episode_body\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is interested in buying a pair of shoes\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      8\u001b[0m     source\u001b[38;5;241m=\u001b[39mEpisodeType\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m      9\u001b[0m     reference_time\u001b[38;5;241m=\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow(timezone\u001b[38;5;241m.\u001b[39mutc),\n\u001b[1;32m     10\u001b[0m     source_description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSalesBot\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# let's get Jess's node uuid\u001b[39;00m\n\u001b[1;32m     14\u001b[0m nl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39m_search(user_name, NODE_HYBRID_SEARCH_EPISODE_MENTIONS)\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/graphiti_core/graphiti.py:824\u001b[0m, in \u001b[0;36mGraphiti.add_episode\u001b[0;34m(self, name, episode_body, source_description, reference_time, source, group_id, uuid, update_communities, entity_types, excluded_entity_types, previous_episode_uuids, edge_types, edge_type_map)\u001b[0m\n\u001b[1;32m    822\u001b[0m span\u001b[38;5;241m.\u001b[39mset_status(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[1;32m    823\u001b[0m span\u001b[38;5;241m.\u001b[39mrecord_exception(e)\n\u001b[0;32m--> 824\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/graphiti_core/graphiti.py:741\u001b[0m, in \u001b[0;36mGraphiti.add_episode\u001b[0;34m(self, name, episode_body, source_description, reference_time, source, group_id, uuid, update_communities, entity_types, excluded_entity_types, previous_episode_uuids, edge_types, edge_type_map)\u001b[0m\n\u001b[1;32m    734\u001b[0m edge_type_map_default \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    735\u001b[0m     {(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntity\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[38;5;28mlist\u001b[39m(edge_types\u001b[38;5;241m.\u001b[39mkeys())}\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m edge_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m {(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntity\u001b[39m\u001b[38;5;124m'\u001b[39m): []}\n\u001b[1;32m    738\u001b[0m )\n\u001b[1;32m    740\u001b[0m \u001b[38;5;66;03m# Extract and resolve nodes\u001b[39;00m\n\u001b[0;32m--> 741\u001b[0m extracted_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m extract_nodes(\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclients, episode, previous_episodes, entity_types, excluded_entity_types\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    745\u001b[0m nodes, uuid_map, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m resolve_extracted_nodes(\n\u001b[1;32m    746\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclients,\n\u001b[1;32m    747\u001b[0m     extracted_nodes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    750\u001b[0m     entity_types,\n\u001b[1;32m    751\u001b[0m )\n\u001b[1;32m    753\u001b[0m \u001b[38;5;66;03m# Extract and resolve edges in parallel with attribute extraction\u001b[39;00m\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/graphiti_core/utils/maintenance/node_operations.py:141\u001b[0m, in \u001b[0;36mextract_nodes\u001b[0;34m(clients, episode, previous_episodes, entity_types, excluded_entity_types)\u001b[0m\n\u001b[1;32m    134\u001b[0m     llm_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m llm_client\u001b[38;5;241m.\u001b[39mgenerate_response(\n\u001b[1;32m    135\u001b[0m         prompt_library\u001b[38;5;241m.\u001b[39mextract_nodes\u001b[38;5;241m.\u001b[39mextract_message(context),\n\u001b[1;32m    136\u001b[0m         response_model\u001b[38;5;241m=\u001b[39mExtractedEntities,\n\u001b[1;32m    137\u001b[0m         group_id\u001b[38;5;241m=\u001b[39mepisode\u001b[38;5;241m.\u001b[39mgroup_id,\n\u001b[1;32m    138\u001b[0m         prompt_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextract_nodes.extract_message\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m episode\u001b[38;5;241m.\u001b[39msource \u001b[38;5;241m==\u001b[39m EpisodeType\u001b[38;5;241m.\u001b[39mtext:\n\u001b[0;32m--> 141\u001b[0m     llm_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m llm_client\u001b[38;5;241m.\u001b[39mgenerate_response(\n\u001b[1;32m    142\u001b[0m         prompt_library\u001b[38;5;241m.\u001b[39mextract_nodes\u001b[38;5;241m.\u001b[39mextract_text(context),\n\u001b[1;32m    143\u001b[0m         response_model\u001b[38;5;241m=\u001b[39mExtractedEntities,\n\u001b[1;32m    144\u001b[0m         group_id\u001b[38;5;241m=\u001b[39mepisode\u001b[38;5;241m.\u001b[39mgroup_id,\n\u001b[1;32m    145\u001b[0m         prompt_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextract_nodes.extract_text\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    146\u001b[0m     )\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m episode\u001b[38;5;241m.\u001b[39msource \u001b[38;5;241m==\u001b[39m EpisodeType\u001b[38;5;241m.\u001b[39mjson:\n\u001b[1;32m    148\u001b[0m     llm_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m llm_client\u001b[38;5;241m.\u001b[39mgenerate_response(\n\u001b[1;32m    149\u001b[0m         prompt_library\u001b[38;5;241m.\u001b[39mextract_nodes\u001b[38;5;241m.\u001b[39mextract_json(context),\n\u001b[1;32m    150\u001b[0m         response_model\u001b[38;5;241m=\u001b[39mExtractedEntities,\n\u001b[1;32m    151\u001b[0m         group_id\u001b[38;5;241m=\u001b[39mepisode\u001b[38;5;241m.\u001b[39mgroup_id,\n\u001b[1;32m    152\u001b[0m         prompt_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextract_nodes.extract_json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    153\u001b[0m     )\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/graphiti_core/llm_client/openai_base_client.py:216\u001b[0m, in \u001b[0;36mBaseOpenAIClient.generate_response\u001b[0;34m(self, messages, response_model, max_tokens, model_size, group_id, prompt_name)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m retry_count \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_RETRIES:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 216\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_response(\n\u001b[1;32m    217\u001b[0m             messages, response_model, max_tokens, model_size\n\u001b[1;32m    218\u001b[0m         )\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (RateLimitError, RefusalError):\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;66;03m# These errors should not trigger retries\u001b[39;00m\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/graphiti_core/llm_client/openai_base_client.py:145\u001b[0m, in \u001b[0;36mBaseOpenAIClient._generate_response\u001b[0;34m(self, messages, response_model, max_tokens, model_size)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response_model:\n\u001b[0;32m--> 145\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_structured_completion(\n\u001b[1;32m    146\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    147\u001b[0m             messages\u001b[38;5;241m=\u001b[39mopenai_messages,\n\u001b[1;32m    148\u001b[0m             temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemperature,\n\u001b[1;32m    149\u001b[0m             max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_tokens,\n\u001b[1;32m    150\u001b[0m             response_model\u001b[38;5;241m=\u001b[39mresponse_model,\n\u001b[1;32m    151\u001b[0m             reasoning\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreasoning,\n\u001b[1;32m    152\u001b[0m             verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbosity,\n\u001b[1;32m    153\u001b[0m         )\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_structured_response(response)\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/graphiti_core/llm_client/openai_client.py:81\u001b[0m, in \u001b[0;36mOpenAIClient._create_structured_completion\u001b[0;34m(self, model, messages, temperature, max_tokens, response_model, reasoning, verbosity)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Reasoning models (gpt-5 family) don't support temperature\u001b[39;00m\n\u001b[1;32m     77\u001b[0m is_reasoning_model \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     78\u001b[0m     model\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-5\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m model\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo1\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m model\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     79\u001b[0m )\n\u001b[0;32m---> 81\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mresponses\u001b[38;5;241m.\u001b[39mparse(\n\u001b[1;32m     82\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mmessages,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_reasoning_model \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     85\u001b[0m     max_output_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[1;32m     86\u001b[0m     text_format\u001b[38;5;241m=\u001b[39mresponse_model,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     reasoning\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meffort\u001b[39m\u001b[38;5;124m'\u001b[39m: reasoning} \u001b[38;5;28;01mif\u001b[39;00m reasoning \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     text\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m'\u001b[39m: verbosity} \u001b[38;5;28;01mif\u001b[39;00m verbosity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     89\u001b[0m )\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/openai/resources/responses/responses.py:2792\u001b[0m, in \u001b[0;36mAsyncResponses.parse\u001b[0;34m(self, text_format, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, prompt_cache_retention, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, verbosity, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   2785\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparser\u001b[39m(raw_response: Response) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ParsedResponse[TextFormatT]:\n\u001b[1;32m   2786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parse_response(\n\u001b[1;32m   2787\u001b[0m         input_tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m   2788\u001b[0m         text_format\u001b[38;5;241m=\u001b[39mtext_format,\n\u001b[1;32m   2789\u001b[0m         response\u001b[38;5;241m=\u001b[39mraw_response,\n\u001b[1;32m   2790\u001b[0m     )\n\u001b[0;32m-> 2792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   2793\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/responses\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2794\u001b[0m     body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m   2795\u001b[0m         {\n\u001b[1;32m   2796\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackground\u001b[39m\u001b[38;5;124m\"\u001b[39m: background,\n\u001b[1;32m   2797\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversation\u001b[39m\u001b[38;5;124m\"\u001b[39m: conversation,\n\u001b[1;32m   2798\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude\u001b[39m\u001b[38;5;124m\"\u001b[39m: include,\n\u001b[1;32m   2799\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2800\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m\"\u001b[39m: instructions,\n\u001b[1;32m   2801\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_output_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_output_tokens,\n\u001b[1;32m   2802\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tool_calls,\n\u001b[1;32m   2803\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   2804\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   2805\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   2806\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_response_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: previous_response_id,\n\u001b[1;32m   2807\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[1;32m   2808\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_cache_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_cache_key,\n\u001b[1;32m   2809\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_cache_retention\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_cache_retention,\n\u001b[1;32m   2810\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning,\n\u001b[1;32m   2811\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafety_identifier\u001b[39m\u001b[38;5;124m\"\u001b[39m: safety_identifier,\n\u001b[1;32m   2812\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   2813\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   2814\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   2815\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   2816\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   2817\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: text,\n\u001b[1;32m   2818\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   2819\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   2820\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   2821\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   2822\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncation\u001b[39m\u001b[38;5;124m\"\u001b[39m: truncation,\n\u001b[1;32m   2823\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   2824\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m\"\u001b[39m: verbosity,\n\u001b[1;32m   2825\u001b[0m         },\n\u001b[1;32m   2826\u001b[0m         response_create_params\u001b[38;5;241m.\u001b[39mResponseCreateParams,\n\u001b[1;32m   2827\u001b[0m     ),\n\u001b[1;32m   2828\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   2829\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers,\n\u001b[1;32m   2830\u001b[0m         extra_query\u001b[38;5;241m=\u001b[39mextra_query,\n\u001b[1;32m   2831\u001b[0m         extra_body\u001b[38;5;241m=\u001b[39mextra_body,\n\u001b[1;32m   2832\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   2833\u001b[0m         post_parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[1;32m   2834\u001b[0m     ),\n\u001b[1;32m   2835\u001b[0m     \u001b[38;5;66;03m# we turn the `Response` instance into a `ParsedResponse`\u001b[39;00m\n\u001b[1;32m   2836\u001b[0m     \u001b[38;5;66;03m# in the `parser` function above\u001b[39;00m\n\u001b[1;32m   2837\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast(Type[ParsedResponse[TextFormatT]], Response),\n\u001b[1;32m   2838\u001b[0m )\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/openai/_base_client.py:1794\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1782\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1789\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1790\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1791\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1792\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1793\u001b[0m     )\n\u001b[0;32m-> 1794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/openai/_base_client.py:1594\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1591\u001b[0m             \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1593\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1594\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'detail': 'Not Found'}"
     ]
    }
   ],
   "source": [
    "from graphiti_core.search.search_config_recipes import NODE_HYBRID_SEARCH_EPISODE_MENTIONS\n",
    "\n",
    "user_name = 'jess'\n",
    "\n",
    "await client.add_episode(\n",
    "    name='User Creation',\n",
    "    episode_body=(f'{user_name} is interested in buying a pair of shoes'),\n",
    "    source=EpisodeType.text,\n",
    "    reference_time=datetime.now(timezone.utc),\n",
    "    source_description='SalesBot',\n",
    ")\n",
    "\n",
    "# let's get Jess's node uuid\n",
    "nl = await client._search(user_name, NODE_HYBRID_SEARCH_EPISODE_MENTIONS)\n",
    "\n",
    "user_node_uuid = nl.nodes[0].uuid\n",
    "\n",
    "# and the ManyBirds node uuid\n",
    "nl = await client._search('ManyBirds', NODE_HYBRID_SEARCH_EPISODE_MENTIONS)\n",
    "manybirds_node_uuid = nl.nodes[0].uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edges_to_facts_string(entities: list[EntityEdge]):\n",
    "    return '-' + '\\n- '.join([edge.fact for edge in entities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, add_messages\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `get_shoe_data` Tool\n",
    "\n",
    "The agent will use this to search the Graphiti graph for information about shoes. We center the search on the `manybirds_node_uuid` to ensure we rank shoe-related data over user data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "async def get_shoe_data(query: str) -> str:\n",
    "    \"\"\"Search the graphiti graph for information about shoes\"\"\"\n",
    "    edge_results = await client.search(\n",
    "        query,\n",
    "        center_node_uuid=manybirds_node_uuid,\n",
    "        num_results=10,\n",
    "    )\n",
    "    return edges_to_facts_string(edge_results)\n",
    "\n",
    "\n",
    "tools = [get_shoe_data]\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='gpt-4.1-mini', temperature=0).bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'detail': 'Not Found'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test the tool node\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m tool_node\u001b[38;5;241m.\u001b[39mainvoke({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;01mawait\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwool shoes\u001b[39m\u001b[38;5;124m'\u001b[39m)]})\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:5561\u001b[0m, in \u001b[0;36mRunnableBindingBase.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5554\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   5555\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m   5556\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5559\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5560\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m   5562\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   5563\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   5564\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   5565\u001b[0m     )\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:421\u001b[0m, in \u001b[0;36mBaseChatModel.ainvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    419\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AIMessage:\n\u001b[1;32m    420\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m--> 421\u001b[0m     llm_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate_prompt(\n\u001b[1;32m    422\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    423\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    424\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    425\u001b[0m         tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    426\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    427\u001b[0m         run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    428\u001b[0m         run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    430\u001b[0m     )\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAIMessage\u001b[39m\u001b[38;5;124m\"\u001b[39m, cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m, llm_result\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m    433\u001b[0m     )\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1128\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21magenerate_prompt\u001b[39m(\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1126\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1127\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate(\n\u001b[1;32m   1129\u001b[0m         prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1130\u001b[0m     )\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1086\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m   1074\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m   1075\u001b[0m             \u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m   1076\u001b[0m                 run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1084\u001b[0m             ]\n\u001b[1;32m   1085\u001b[0m         )\n\u001b[0;32m-> 1086\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1087\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1088\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m   1090\u001b[0m ]\n\u001b[1;32m   1091\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1339\u001b[0m, in \u001b[0;36mBaseChatModel._agenerate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1339\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(\n\u001b[1;32m   1340\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1341\u001b[0m     )\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1343\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1630\u001b[0m, in \u001b[0;36mBaseChatOpenAI._agenerate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_response\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1629\u001b[0m         e\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mhttp_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_response_headers\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1634\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1635\u001b[0m ):\n\u001b[1;32m   1636\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1623\u001b[0m, in \u001b[0;36mBaseChatOpenAI._agenerate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1616\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[1;32m   1617\u001b[0m             response,\n\u001b[1;32m   1618\u001b[0m             schema\u001b[38;5;241m=\u001b[39moriginal_schema_obj,\n\u001b[1;32m   1619\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m   1620\u001b[0m             output_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_version,\n\u001b[1;32m   1621\u001b[0m         )\n\u001b[1;32m   1622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1623\u001b[0m         raw_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client\u001b[38;5;241m.\u001b[39mwith_raw_response\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m   1624\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload\n\u001b[1;32m   1625\u001b[0m         )\n\u001b[1;32m   1626\u001b[0m         response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m   1627\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/openai/_legacy_response.py:381\u001b[0m, in \u001b[0;36masync_to_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m extra_headers[RAW_RESPONSE_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    379\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:2678\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   2631\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   2632\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   2633\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2675\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m not_given,\n\u001b[1;32m   2676\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[1;32m   2677\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 2678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   2679\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2680\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   2681\u001b[0m             {\n\u001b[1;32m   2682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   2683\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   2684\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   2685\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   2686\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   2687\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   2688\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   2689\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   2690\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   2691\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   2692\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   2693\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   2694\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   2695\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   2696\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m   2697\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   2698\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_cache_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_cache_key,\n\u001b[1;32m   2699\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_cache_retention\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_cache_retention,\n\u001b[1;32m   2700\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m   2701\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   2702\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafety_identifier\u001b[39m\u001b[38;5;124m\"\u001b[39m: safety_identifier,\n\u001b[1;32m   2703\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   2704\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   2705\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   2706\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   2707\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   2708\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   2709\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   2710\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   2711\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   2712\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   2713\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   2714\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   2715\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m\"\u001b[39m: verbosity,\n\u001b[1;32m   2716\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[1;32m   2717\u001b[0m             },\n\u001b[1;32m   2718\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsStreaming\n\u001b[1;32m   2719\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[1;32m   2720\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsNonStreaming,\n\u001b[1;32m   2721\u001b[0m         ),\n\u001b[1;32m   2722\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   2723\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   2724\u001b[0m         ),\n\u001b[1;32m   2725\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   2726\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2727\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   2728\u001b[0m     )\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/openai/_base_client.py:1794\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1782\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1789\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1790\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m   1791\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1792\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1793\u001b[0m     )\n\u001b[0;32m-> 1794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m~/muhammad_kashif/graphiti/.venv/lib/python3.10/site-packages/openai/_base_client.py:1594\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1591\u001b[0m             \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1593\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1594\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'detail': 'Not Found'}"
     ]
    }
   ],
   "source": [
    "# Test the tool node\n",
    "await tool_node.ainvoke({'messages': [await llm.ainvoke('wool shoes')]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot Function Explanation\n",
    "\n",
    "The chatbot uses Graphiti to provide context-aware responses in a shoe sales scenario. Here's how it works:\n",
    "\n",
    "1. **Context Retrieval**: It searches the Graphiti graph for relevant information based on the latest message, using the user's node as the center point. This ensures that user-related facts are ranked higher than other information in the graph.\n",
    "\n",
    "2. **System Message**: It constructs a system message incorporating facts from Graphiti, setting the context for the AI's response.\n",
    "\n",
    "3. **Knowledge Persistence**: After generating a response, it asynchronously adds the interaction to the Graphiti graph, allowing future queries to reference this conversation.\n",
    "\n",
    "This approach enables the chatbot to maintain context across interactions and provide personalized responses based on the user's history and preferences stored in the Graphiti graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_name: str\n",
    "    user_node_uuid: str\n",
    "\n",
    "\n",
    "async def chatbot(state: State):\n",
    "    facts_string = None\n",
    "    if len(state['messages']) > 0:\n",
    "        last_message = state['messages'][-1]\n",
    "        graphiti_query = f'{\"SalesBot\" if isinstance(last_message, AIMessage) else state[\"user_name\"]}: {last_message.content}'\n",
    "        # search graphiti using Jess's node uuid as the center node\n",
    "        # graph edges (facts) further from the Jess node will be ranked lower\n",
    "        edge_results = await client.search(\n",
    "            graphiti_query, center_node_uuid=state['user_node_uuid'], num_results=5\n",
    "        )\n",
    "        facts_string = edges_to_facts_string(edge_results)\n",
    "\n",
    "    system_message = SystemMessage(\n",
    "        content=f\"\"\"You are a skillfull shoe salesperson working for ManyBirds. Review information about the user and their prior conversation below and respond accordingly.\n",
    "        Keep responses short and concise. And remember, always be selling (and helpful!)\n",
    "\n",
    "        Things you'll need to know about the user in order to close a sale:\n",
    "        - the user's shoe size\n",
    "        - any other shoe needs? maybe for wide feet?\n",
    "        - the user's preferred colors and styles\n",
    "        - their budget\n",
    "\n",
    "        Ensure that you ask the user for the above if you don't already know.\n",
    "\n",
    "        Facts about the user and their conversation:\n",
    "        {facts_string or 'No facts about the user and their conversation'}\"\"\"\n",
    "    )\n",
    "\n",
    "    messages = [system_message] + state['messages']\n",
    "\n",
    "    response = await llm.ainvoke(messages)\n",
    "\n",
    "    # add the response to the graphiti graph.\n",
    "    # this will allow us to use the graphiti search later in the conversation\n",
    "    # we're doing async here to avoid blocking the graph execution\n",
    "    asyncio.create_task(\n",
    "        client.add_episode(\n",
    "            name='Chatbot Response',\n",
    "            episode_body=f'{state[\"user_name\"]}: {state[\"messages\"][-1]}\\nSalesBot: {response.content}',\n",
    "            source=EpisodeType.message,\n",
    "            reference_time=datetime.now(timezone.utc),\n",
    "            source_description='Chatbot',\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return {'messages': [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Agent\n",
    "\n",
    "This section sets up the Agent's LangGraph graph:\n",
    "\n",
    "1. **Graph Structure**: It defines a graph with nodes for the agent (chatbot) and tools, connected in a loop.\n",
    "\n",
    "2. **Conditional Logic**: The `should_continue` function determines whether to end the graph execution or continue to the tools node based on the presence of tool calls.\n",
    "\n",
    "3. **Memory Management**: It uses a MemorySaver to maintain conversation state across turns. This is in addition to using Graphiti for facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "async def should_continue(state, config):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return 'end'\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return 'continue'\n",
    "\n",
    "\n",
    "graph_builder.add_node('agent', chatbot)\n",
    "graph_builder.add_node('tools', tool_node)\n",
    "\n",
    "graph_builder.add_edge(START, 'agent')\n",
    "graph_builder.add_conditional_edges('agent', should_continue, {'continue': 'tools', 'end': END})\n",
    "graph_builder.add_edge('tools', 'agent')\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our LangGraph agent graph is illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with suppress(Exception):\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Agent\n",
    "\n",
    "Let's test the agent with a single call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await graph.ainvoke(\n",
    "    {\n",
    "        'messages': [\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': 'What sizes do the TinyBirds Wool Runners in Natural Black come in?',\n",
    "            }\n",
    "        ],\n",
    "        'user_name': user_name,\n",
    "        'user_node_uuid': user_node_uuid,\n",
    "    },\n",
    "    config={'configurable': {'thread_id': uuid.uuid4().hex}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the Graph\n",
    "\n",
    "At this stage, the graph would look something like this. The `jess` node is `INTERESTED_IN` the `TinyBirds Wool Runner` node. The image below was generated using Neo4j Desktop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='tinybirds-jess.png', width=850))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Agent interactively\n",
    "\n",
    "The following code will run the agent in an event loop. Just enter a message into the box and click submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_output = widgets.Output()\n",
    "config = {'configurable': {'thread_id': uuid.uuid4().hex}}\n",
    "user_state = {'user_name': user_name, 'user_node_uuid': user_node_uuid}\n",
    "\n",
    "\n",
    "async def process_input(user_state: State, user_input: str):\n",
    "    conversation_output.append_stdout(f'\\nUser: {user_input}\\n')\n",
    "    conversation_output.append_stdout('\\nAssistant: ')\n",
    "\n",
    "    graph_state = {\n",
    "        'messages': [{'role': 'user', 'content': user_input}],\n",
    "        'user_name': user_state['user_name'],\n",
    "        'user_node_uuid': user_state['user_node_uuid'],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        async for event in graph.astream(\n",
    "            graph_state,\n",
    "            config=config,\n",
    "        ):\n",
    "            for value in event.values():\n",
    "                if 'messages' in value:\n",
    "                    last_message = value['messages'][-1]\n",
    "                    if isinstance(last_message, AIMessage) and isinstance(\n",
    "                        last_message.content, str\n",
    "                    ):\n",
    "                        conversation_output.append_stdout(last_message.content)\n",
    "    except Exception as e:\n",
    "        conversation_output.append_stdout(f'Error: {e}')\n",
    "\n",
    "\n",
    "def on_submit(b):\n",
    "    user_input = input_box.value\n",
    "    input_box.value = ''\n",
    "    asyncio.create_task(process_input(user_state, user_input))\n",
    "\n",
    "\n",
    "input_box = widgets.Text(placeholder='Type your message here...')\n",
    "submit_button = widgets.Button(description='Send')\n",
    "submit_button.on_click(on_submit)\n",
    "\n",
    "conversation_output.append_stdout('Assistant: Hello, how can I help you find shoes today?')\n",
    "\n",
    "display(widgets.VBox([input_box, submit_button, conversation_output]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
